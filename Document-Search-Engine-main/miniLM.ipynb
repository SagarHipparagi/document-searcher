{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üîç Multi-Document Search Engine with RAG\n",
                "\n",
                "This notebook implements a complete RAG (Retrieval-Augmented Generation) system that:\n",
                "- Searches across PDF, DOCX, and CSV documents\n",
                "- Uses intelligent routing to determine document type\n",
                "- Provides conversational answers using LLM\n",
                "\n",
                "**Model Details:**\n",
                "- Embeddings: `sentence-transformers/all-mpnet-base-v2` (768 dimensions)\n",
                "- LLM: `openai/gpt-oss-20b:free` via OpenRouter"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Load Environment Variables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Environment variables loaded\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "from dotenv import load_dotenv, find_dotenv\n",
                "\n",
                "# Load API keys from .env file\n",
                "_ = load_dotenv(find_dotenv())\n",
                "\n",
                "print(\"‚úÖ Environment variables loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Import Required Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "WARNING:tensorflow:From c:\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
                        "\n",
                        "‚úÖ All libraries imported successfully\n"
                    ]
                }
            ],
            "source": [
                "# LangChain core\n",
                "from langchain_openai import ChatOpenAI\n",
                "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
                "from langchain_community.document_loaders import CSVLoader, PyPDFLoader, Docx2txtLoader\n",
                "\n",
                "# LangChain core components\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from langchain_core.documents import Document\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "from langchain_core.runnables import RunnablePassthrough\n",
                "\n",
                "# HuggingFace for embeddings\n",
                "from langchain_huggingface import HuggingFaceEmbeddings\n",
                "\n",
                "# Utilities\n",
                "import glob\n",
                "from typing import List, Dict\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"‚úÖ All libraries imported successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Initialize Embeddings Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading embeddings model... (this may take a minute)\n"
                    ]
                }
            ],
            "source": [
                "print(\"Loading embeddings model... (this may take a minute)\")\n",
                "\n",
                "embeddings = HuggingFaceEmbeddings(\n",
                "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
                "    encode_kwargs={\"normalize_embeddings\": True},  # for cosine similarity\n",
                ")\n",
                "\n",
                "# Test the embeddings\n",
                "test_text = \"Hello World, how are you?\"\n",
                "test_embedding = embeddings.embed_query(test_text)\n",
                "print(f\"‚úÖ Embedding model loaded successfully!\")\n",
                "print(f\"   Embedding dimension: {len(test_embedding)}\")\n",
                "print(f\"   Sample values: {test_embedding[:5]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Initialize LLM (OpenRouter)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ LLM initialized successfully!\n",
                        "   Model: openai/gpt-oss-20b:free\n",
                        "   Provider: OpenRouter\n"
                    ]
                }
            ],
            "source": [
                "# Initialize the LLM for routing and answering\n",
                "llm = ChatOpenAI(\n",
                "    temperature=0.0,\n",
                "    base_url=\"https://openrouter.ai/api/v1\",\n",
                "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
                "    model=\"openai/gpt-oss-20b:free\",\n",
                ")\n",
                "\n",
                "print(\"‚úÖ LLM initialized successfully!\")\n",
                "print(f\"   Model: openai/gpt-oss-20b:free\")\n",
                "print(f\"   Provider: OpenRouter\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Load Documents"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "Loading documents...\n",
                        "============================================================\n",
                        "üìÑ Loading PDF: .\\iphone17.pdf\n",
                        "üìù Loading DOCX: .\\f1info.docx\n",
                        "üìä Loading CSV: .\\sales.csv\n",
                        "\n",
                        "============================================================\n",
                        "Document Loading Summary\n",
                        "============================================================\n",
                        "  PDF: 7 documents\n",
                        "  DOCX: 1 documents\n",
                        "  CSV: 299 documents\n",
                        "\n",
                        "‚úÖ Total documents loaded: 307\n"
                    ]
                }
            ],
            "source": [
                "def load_documents_by_type(directory: str = \".\") -> Dict[str, List[Document]]:\n",
                "    \"\"\"\n",
                "    Load all documents from directory, organized by type\n",
                "    Returns a dictionary with keys: 'pdf', 'docx', 'csv'\n",
                "    \"\"\"\n",
                "    documents_by_type = {\n",
                "        'pdf': [],\n",
                "        'docx': [],\n",
                "        'csv': []\n",
                "    }\n",
                "    \n",
                "    # Load PDF files\n",
                "    pdf_files = glob.glob(f\"{directory}/*.pdf\")\n",
                "    for pdf_file in pdf_files:\n",
                "        print(f\"üìÑ Loading PDF: {pdf_file}\")\n",
                "        loader = PyPDFLoader(pdf_file)\n",
                "        docs = loader.load()\n",
                "        for doc in docs:\n",
                "            doc.metadata['doc_type'] = 'pdf'\n",
                "        documents_by_type['pdf'].extend(docs)\n",
                "    \n",
                "    # Load DOCX files\n",
                "    docx_files = glob.glob(f\"{directory}/*.docx\")\n",
                "    for docx_file in docx_files:\n",
                "        print(f\"üìù Loading DOCX: {docx_file}\")\n",
                "        loader = Docx2txtLoader(docx_file)\n",
                "        docs = loader.load()\n",
                "        for doc in docs:\n",
                "            doc.metadata['doc_type'] = 'docx'\n",
                "        documents_by_type['docx'].extend(docs)\n",
                "    \n",
                "    # Load CSV files\n",
                "    csv_files = glob.glob(f\"{directory}/*.csv\")\n",
                "    for csv_file in csv_files:\n",
                "        print(f\"üìä Loading CSV: {csv_file}\")\n",
                "        loader = CSVLoader(file_path=csv_file)\n",
                "        docs = loader.load()\n",
                "        for doc in docs:\n",
                "            doc.metadata['doc_type'] = 'csv'\n",
                "        documents_by_type['csv'].extend(docs)\n",
                "    \n",
                "    return documents_by_type\n",
                "\n",
                "# Load all documents\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"Loading documents...\")\n",
                "print(\"=\"*60)\n",
                "all_documents = load_documents_by_type(\".\")\n",
                "\n",
                "# Print summary\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"Document Loading Summary\")\n",
                "print(\"=\"*60)\n",
                "for doc_type, docs in all_documents.items():\n",
                "    if docs:\n",
                "        print(f\"  {doc_type.upper()}: {len(docs)} documents\")\n",
                "print(f\"\\n‚úÖ Total documents loaded: {sum(len(docs) for docs in all_documents.values())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Create Vector Stores"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Creating vector stores...\n",
                        "  üî® Creating PDF vector store (7 docs)\n",
                        "  üî® Creating DOCX vector store (1 docs)\n",
                        "  üî® Creating CSV vector store (299 docs)\n",
                        "\n",
                        "‚úÖ Vector stores created for: ['pdf', 'docx', 'csv']\n"
                    ]
                }
            ],
            "source": [
                "print(\"Creating vector stores...\")\n",
                "vector_stores = {}\n",
                "retrievers = {}\n",
                "\n",
                "# Create vector store for PDFs\n",
                "if all_documents['pdf']:\n",
                "    print(f\"  üî® Creating PDF vector store ({len(all_documents['pdf'])} docs)\")\n",
                "    vector_stores['pdf'] = DocArrayInMemorySearch.from_documents(\n",
                "        all_documents['pdf'], \n",
                "        embeddings\n",
                "    )\n",
                "    retrievers['pdf'] = vector_stores['pdf'].as_retriever(search_kwargs={\"k\": 5})\n",
                "\n",
                "# Create vector store for DOCX files\n",
                "if all_documents['docx']:\n",
                "    print(f\"  üî® Creating DOCX vector store ({len(all_documents['docx'])} docs)\")\n",
                "    vector_stores['docx'] = DocArrayInMemorySearch.from_documents(\n",
                "        all_documents['docx'], \n",
                "        embeddings\n",
                "    )\n",
                "    retrievers['docx'] = vector_stores['docx'].as_retriever(search_kwargs={\"k\": 5})\n",
                "\n",
                "# Create vector store for CSV files\n",
                "if all_documents['csv']:\n",
                "    print(f\"  üî® Creating CSV vector store ({len(all_documents['csv'])} docs)\")\n",
                "    vector_stores['csv'] = DocArrayInMemorySearch.from_documents(\n",
                "        all_documents['csv'], \n",
                "        embeddings\n",
                "    )\n",
                "    retrievers['csv'] = vector_stores['csv'].as_retriever(search_kwargs={\"k\": 10})\n",
                "\n",
                "print(f\"\\n‚úÖ Vector stores created for: {list(retrievers.keys())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Create QA Chains"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Creating QA chains...\n",
                        "  ‚õìÔ∏è  Creating QA chain for PDF documents\n",
                        "  ‚õìÔ∏è  Creating QA chain for DOCX documents\n",
                        "  ‚õìÔ∏è  Creating QA chain for CSV documents\n",
                        "\n",
                        "‚úÖ QA chains created for: ['pdf', 'docx', 'csv']\n"
                    ]
                }
            ],
            "source": [
                "print(\"Creating QA chains...\")\n",
                "qa_chains = {}\n",
                "\n",
                "# Create a prompt template for QA\n",
                "qa_prompt = ChatPromptTemplate.from_template(\n",
                "    \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know.\n",
                "\n",
                "Question: {question}\n",
                "\n",
                "Context: {context}\n",
                "\n",
                "Answer:\"\"\"\n",
                ")\n",
                "\n",
                "# Helper function to format docs\n",
                "def format_docs(docs):\n",
                "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
                "\n",
                "for doc_type, retriever in retrievers.items():\n",
                "    print(f\"  ‚õìÔ∏è  Creating QA chain for {doc_type.upper()} documents\")\n",
                "    \n",
                "    # Create a simple RAG chain using LCEL (LangChain Expression Language)\n",
                "    qa_chains[doc_type] = (\n",
                "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
                "        | qa_prompt\n",
                "        | llm\n",
                "        | StrOutputParser()\n",
                "    )\n",
                "\n",
                "print(f\"\\n‚úÖ QA chains created for: {list(qa_chains.keys())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Configure Router"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Router configured with 3 document types\n",
                        "   - pdf: The PDF document contains information about iPhone 17 series launch (September 2...\n",
                        "   - docx: The Word document contains information about F1 Singapore Grand Prix 2025 (Octob...\n",
                        "   - csv: The CSV document contains sales data with customer orders. Good for answering qu...\n"
                    ]
                }
            ],
            "source": [
                "# Define retriever information for the router\n",
                "retriever_infos = []\n",
                "\n",
                "if 'pdf' in retrievers:\n",
                "    retriever_infos.append({\n",
                "        \"name\": \"pdf\",\n",
                "        \"description\": \"The PDF document contains information about iPhone 17 series launch (September 2025). Good for answering questions about iPhone 17 features, specifications, launch dates, pricing, and product details.\",\n",
                "        \"retriever\": retrievers['pdf']\n",
                "    })\n",
                "\n",
                "if 'docx' in retrievers:\n",
                "    retriever_infos.append({\n",
                "        \"name\": \"docx\",\n",
                "        \"description\": \"The Word document contains information about F1 Singapore Grand Prix 2025 (October 2025). Good for answering questions about F1 race, Grand Prix details, race results, and Singapore event information.\",\n",
                "        \"retriever\": retrievers['docx']\n",
                "    })\n",
                "\n",
                "if 'csv' in retrievers:\n",
                "    retriever_infos.append({\n",
                "        \"name\": \"csv\",\n",
                "        \"description\": \"The CSV document contains sales data with customer orders. Good for answering questions about sales records, orders, corporate segment, customer information, numerical data, and transaction details.\",\n",
                "        \"retriever\": retrievers['csv']\n",
                "    })\n",
                "\n",
                "print(f\"‚úÖ Router configured with {len(retriever_infos)} document types\")\n",
                "for info in retriever_infos:\n",
                "    print(f\"   - {info['name']}: {info['description'][:80]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Create Multi-Retrieval QA Chain"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Setting up intelligent router...\n",
                        "\n",
                        "============================================================\n",
                        "‚úÖ SYSTEM READY!\n",
                        "============================================================\n",
                        "Available document types: ['pdf', 'docx', 'csv']\n",
                        "\n",
                        "You can now ask questions using: query_documents('your question')\n",
                        "============================================================\n"
                    ]
                }
            ],
            "source": [
                "# Simple routing logic using LLM classification\n",
                "print(\"Setting up intelligent router...\")\n",
                "\n",
                "# Router prompt to determine which document type to search\n",
                "router_template = \"\"\"Given the user question below, classify it to route to the most relevant document type.\n",
                "\n",
                "Available document types:\n",
                "- pdf: iPhone 17 series information (features, specs, launch, pricing)\n",
                "- docx: F1 Singapore Grand Prix 2025 information (race results, event details)\n",
                "- csv: Sales data with orders, customers, segments\n",
                "\n",
                "User question: {question}\n",
                "\n",
                "Respond with ONLY ONE WORD - either 'pdf', 'docx', or 'csv'. Nothing else.\n",
                "Classification:\"\"\"\n",
                "\n",
                "router_prompt = ChatPromptTemplate.from_template(router_template)\n",
                "router_chain = router_prompt | llm | StrOutputParser()\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"‚úÖ SYSTEM READY!\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Available document types: {list(retrievers.keys())}\")\n",
                "print(\"\\nYou can now ask questions using: query_documents('your question')\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 10: Query Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def query_documents(user_query: str):\n",
                "    \"\"\"\n",
                "    Main query function with intelligent routing\n",
                "    \n",
                "    Args:\n",
                "        user_query: Your question (string)\n",
                "        \n",
                "    The router will analyze your question and route it to:\n",
                "        - PDF documents (for iPhone 17 info)\n",
                "        - DOCX documents (for F1 Singapore GP info)\n",
                "        - CSV files (for sales data analysis)\n",
                "    \"\"\"\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"USER QUERY: {user_query}\")\n",
                "    print(f\"{'='*60}\\n\")\n",
                "    \n",
                "    try:\n",
                "        # Use router to determine which document type to search\n",
                "        doc_type = router_chain.invoke({\"question\": user_query}).strip().lower()\n",
                "        \n",
                "        print(f\"üéØ Routing to: {doc_type.upper()} documents\\n\")\n",
                "        \n",
                "        # Validate doc_type\n",
                "        if doc_type not in qa_chains:\n",
                "            print(f\"‚ö†Ô∏è  Warning: '{doc_type}' not found, using first available chain\")\n",
                "            doc_type = list(qa_chains.keys())[0]\n",
                "        \n",
                "        # Run the appropriate QA chain\n",
                "        answer = qa_chains[doc_type].invoke(user_query)\n",
                "        \n",
                "        print(f\"\\n{'='*60}\")\n",
                "        print(\"ANSWER:\")\n",
                "        print(f\"{'='*60}\")\n",
                "        print(answer)\n",
                "        print()\n",
                "        \n",
                "        return answer\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error: {str(e)}\")\n",
                "        import traceback\n",
                "        traceback.print_exc()\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example Queries - Try These!\n",
                "\n",
                "Run the cell below to test the system with example queries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "USER QUERY: Can you list me five corporate segment orders in the sales data?\n",
                        "============================================================\n",
                        "\n",
                        "‚ùå Error: Error code: 401 - {'error': {'message': 'User not found.', 'code': 401}}\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Traceback (most recent call last):\n",
                        "  File \"C:\\Users\\sagar\\AppData\\Local\\Temp\\ipykernel_12452\\749201197.py\", line 19, in query_documents\n",
                        "    doc_type = router_chain.invoke({\"question\": user_query}).strip().lower()\n",
                        "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3151, in invoke\n",
                        "    input_ = context.run(step.invoke, input_, config)\n",
                        "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 402, in invoke\n",
                        "    self.generate_prompt(\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1121, in generate_prompt\n",
                        "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 931, in generate\n",
                        "    self._generate_with_cache(\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1233, in _generate_with_cache\n",
                        "    result = self._generate(\n",
                        "             ^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 1386, in _generate\n",
                        "    raise e\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 1381, in _generate\n",
                        "    raw_response = self.client.with_raw_response.create(**payload)\n",
                        "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\openai\\_legacy_response.py\", line 364, in wrapped\n",
                        "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
                        "                                      ^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 286, in wrapper\n",
                        "    return func(*args, **kwargs)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 1147, in create\n",
                        "    return self._post(\n",
                        "           ^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1259, in post\n",
                        "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
                        "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1047, in request\n",
                        "    raise self._make_status_error_from_response(err.response) from None\n",
                        "openai.AuthenticationError: Error code: 401 - {'error': {'message': 'User not found.', 'code': 401}}\n"
                    ]
                }
            ],
            "source": [
                "# Example 1: CSV Query\n",
                "query_documents(\"Can you list me five corporate segment orders in the sales data?\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "USER QUERY: What are the main features of iPhone 17?\n",
                        "============================================================\n",
                        "\n",
                        "‚ùå Error: Error code: 401 - {'error': {'message': 'User not found.', 'code': 401}}\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Traceback (most recent call last):\n",
                        "  File \"C:\\Users\\sagar\\AppData\\Local\\Temp\\ipykernel_12452\\749201197.py\", line 19, in query_documents\n",
                        "    doc_type = router_chain.invoke({\"question\": user_query}).strip().lower()\n",
                        "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3151, in invoke\n",
                        "    input_ = context.run(step.invoke, input_, config)\n",
                        "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 402, in invoke\n",
                        "    self.generate_prompt(\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1121, in generate_prompt\n",
                        "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 931, in generate\n",
                        "    self._generate_with_cache(\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1233, in _generate_with_cache\n",
                        "    result = self._generate(\n",
                        "             ^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 1386, in _generate\n",
                        "    raise e\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 1381, in _generate\n",
                        "    raw_response = self.client.with_raw_response.create(**payload)\n",
                        "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\openai\\_legacy_response.py\", line 364, in wrapped\n",
                        "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
                        "                                      ^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 286, in wrapper\n",
                        "    return func(*args, **kwargs)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 1147, in create\n",
                        "    return self._post(\n",
                        "           ^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1259, in post\n",
                        "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
                        "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1047, in request\n",
                        "    raise self._make_status_error_from_response(err.response) from None\n",
                        "openai.AuthenticationError: Error code: 401 - {'error': {'message': 'User not found.', 'code': 401}}\n"
                    ]
                }
            ],
            "source": [
                "# Example 2: PDF Query\n",
                "query_documents(\"What are the main features of iPhone 17?\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "USER QUERY: Who won the Singapore Grand Prix 2025?\n",
                        "============================================================\n",
                        "\n",
                        "‚ùå Error: Error code: 401 - {'error': {'message': 'User not found.', 'code': 401}}\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Traceback (most recent call last):\n",
                        "  File \"C:\\Users\\sagar\\AppData\\Local\\Temp\\ipykernel_12452\\749201197.py\", line 19, in query_documents\n",
                        "    doc_type = router_chain.invoke({\"question\": user_query}).strip().lower()\n",
                        "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3151, in invoke\n",
                        "    input_ = context.run(step.invoke, input_, config)\n",
                        "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 402, in invoke\n",
                        "    self.generate_prompt(\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1121, in generate_prompt\n",
                        "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 931, in generate\n",
                        "    self._generate_with_cache(\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1233, in _generate_with_cache\n",
                        "    result = self._generate(\n",
                        "             ^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 1386, in _generate\n",
                        "    raise e\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 1381, in _generate\n",
                        "    raw_response = self.client.with_raw_response.create(**payload)\n",
                        "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\openai\\_legacy_response.py\", line 364, in wrapped\n",
                        "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
                        "                                      ^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 286, in wrapper\n",
                        "    return func(*args, **kwargs)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 1147, in create\n",
                        "    return self._post(\n",
                        "           ^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1259, in post\n",
                        "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
                        "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1047, in request\n",
                        "    raise self._make_status_error_from_response(err.response) from None\n",
                        "openai.AuthenticationError: Error code: 401 - {'error': {'message': 'User not found.', 'code': 401}}\n"
                    ]
                }
            ],
            "source": [
                "# Example 3: DOCX Query\n",
                "query_documents(\"Who won the Singapore Grand Prix 2025?\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Your Custom Queries\n",
                "\n",
                "Use the cell below to ask your own questions!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "USER QUERY: what is iphone\n",
                        "============================================================\n",
                        "\n",
                        "‚ùå Error: Error code: 401 - {'error': {'message': 'User not found.', 'code': 401}}\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Traceback (most recent call last):\n",
                        "  File \"C:\\Users\\sagar\\AppData\\Local\\Temp\\ipykernel_12452\\749201197.py\", line 19, in query_documents\n",
                        "    doc_type = router_chain.invoke({\"question\": user_query}).strip().lower()\n",
                        "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3151, in invoke\n",
                        "    input_ = context.run(step.invoke, input_, config)\n",
                        "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 402, in invoke\n",
                        "    self.generate_prompt(\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1121, in generate_prompt\n",
                        "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 931, in generate\n",
                        "    self._generate_with_cache(\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1233, in _generate_with_cache\n",
                        "    result = self._generate(\n",
                        "             ^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 1386, in _generate\n",
                        "    raise e\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 1381, in _generate\n",
                        "    raw_response = self.client.with_raw_response.create(**payload)\n",
                        "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\openai\\_legacy_response.py\", line 364, in wrapped\n",
                        "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
                        "                                      ^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 286, in wrapper\n",
                        "    return func(*args, **kwargs)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 1147, in create\n",
                        "    return self._post(\n",
                        "           ^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1259, in post\n",
                        "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
                        "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"c:\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1047, in request\n",
                        "    raise self._make_status_error_from_response(err.response) from None\n",
                        "openai.AuthenticationError: Error code: 401 - {'error': {'message': 'User not found.', 'code': 401}}\n"
                    ]
                }
            ],
            "source": [
                "# Ask your own question here\n",
                "my_question = \"what is iphone\"\n",
                "\n",
                "query_documents(my_question)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
